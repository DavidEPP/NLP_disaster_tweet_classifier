{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30527,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\nThis starter notebook is provided by the Keras team.</center>\n\n## Keras NLP starter guide here: https://keras.io/guides/keras_nlp/getting_started/\n\nIn this competition, the challenge is to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.\nA dataset of 10,000 tweets that were hand classified is available. \n\n__This starter notebook uses the [DistilBERT](https://arxiv.org/abs/1910.01108) pretrained model from KerasNLP.__\n\n\n**BERT** stands for **Bidirectional Encoder Representations from Transformers**. BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models.\n\nThe BERT family of models uses the **Transformer encoder architecture** to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.\n\nBERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n\n**DistilBERT model** is a distilled form of the **BERT** model. The size of a BERT model was reduced by 40% via knowledge distillation during the pre-training phase while retaining 97% of its language understanding abilities and being 60% faster.\n\n\n\n![BERT Architecture](https://www.cse.chalmers.se/~richajo/nlp2019/l5/bert_class.png)\n\n\n\nIn this notebook, you will:\n\n- Load the Disaster Tweets\n- Explore the dataset\n- Preprocess the data\n- Load a DistilBERT model from Keras NLP\n- Train your own model, fine-tuning BERT\n- Generate the submission file\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport multiprocessing as mp\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-01-28T14:56:52.877579Z","iopub.execute_input":"2025-01-28T14:56:52.877945Z","iopub.status.idle":"2025-01-28T14:56:53.227167Z","shell.execute_reply.started":"2025-01-28T14:56:52.877918Z","shell.execute_reply":"2025-01-28T14:56:53.225889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load the Disaster Tweets\nLet's have a look at the train and test dataset.\n\nThey contain:\n- id\n- keyword: A keyword from that tweet (although this may be blank!)\n- location: The location the tweet was sent from (may also be blank)\n- text: The text of a tweet\n- target: 1 if the tweet is a real disaster or 0 if not","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nprint('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))","metadata":{"execution":{"iopub.status.busy":"2025-01-28T14:56:53.229437Z","iopub.execute_input":"2025-01-28T14:56:53.230607Z","iopub.status.idle":"2025-01-28T14:56:53.311001Z","shell.execute_reply.started":"2025-01-28T14:56:53.230568Z","shell.execute_reply":"2025-01-28T14:56:53.309826Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2025-01-28T14:56:53.312299Z","iopub.execute_input":"2025-01-28T14:56:53.312704Z","iopub.status.idle":"2025-01-28T14:56:53.330057Z","shell.execute_reply.started":"2025-01-28T14:56:53.312667Z","shell.execute_reply":"2025-01-28T14:56:53.328590Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2025-01-28T14:56:53.332266Z","iopub.execute_input":"2025-01-28T14:56:53.332618Z","iopub.status.idle":"2025-01-28T14:56:53.343044Z","shell.execute_reply.started":"2025-01-28T14:56:53.332590Z","shell.execute_reply":"2025-01-28T14:56:53.341911Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Explore the dataset","metadata":{}},{"cell_type":"code","source":"df_train[\"length\"] = df_train[\"text\"].apply(lambda x : len(x))\ndf_test[\"length\"] = df_test[\"text\"].apply(lambda x : len(x))\n\nprint(\"Train Length Stat\")\nprint(df_train[\"length\"].describe())\nprint()\n\nprint(\"Test Length Stat\")\nprint(df_test[\"length\"].describe())","metadata":{"execution":{"iopub.status.busy":"2025-01-28T14:56:53.344231Z","iopub.execute_input":"2025-01-28T14:56:53.344577Z","iopub.status.idle":"2025-01-28T14:56:53.372941Z","shell.execute_reply.started":"2025-01-28T14:56:53.344550Z","shell.execute_reply":"2025-01-28T14:56:53.371830Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_train.isna().sum())\nprint(df_test.isna().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:14:05.783778Z","iopub.execute_input":"2025-01-28T16:14:05.784149Z","iopub.status.idle":"2025-01-28T16:14:05.799076Z","shell.execute_reply.started":"2025-01-28T16:14:05.784121Z","shell.execute_reply":"2025-01-28T16:14:05.797954Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can note that some of the tweets do not include a keyword or a location. In the first attempt, those will not be considered.","metadata":{}},{"cell_type":"code","source":"print (f'{len(df_train[\"location\"].unique())} different locations')\nprint (f'{len(df_train[\"keyword\"].unique())} different keywords')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T14:56:53.405164Z","iopub.execute_input":"2025-01-28T14:56:53.405572Z","iopub.status.idle":"2025-01-28T14:56:53.417410Z","shell.execute_reply.started":"2025-01-28T14:56:53.405529Z","shell.execute_reply":"2025-01-28T14:56:53.416410Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can check for inconsistent tags (same text with different target).","metadata":{}},{"cell_type":"code","source":"# Count the duplicated values in the 'Text' column\nduplicated_text_count = df_train.duplicated(subset=['text']).sum()\n\n# Count the duplicated values based on both 'Text' and 'Category'. If the counts are the same as above,those duplicates have consistent 'Category'\nduplicated_textcat_count = df_train.duplicated(subset=['text', 'target']).sum()\n\n# Print the counts\nprint(f'There are {duplicated_text_count} rows with the same texts in the training data.')\nprint(f'There are {duplicated_textcat_count} rows with the same texts and categories in the training data.')\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T14:56:53.419065Z","iopub.execute_input":"2025-01-28T14:56:53.419458Z","iopub.status.idle":"2025-01-28T14:56:53.437886Z","shell.execute_reply.started":"2025-01-28T14:56:53.419422Z","shell.execute_reply":"2025-01-28T14:56:53.436618Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are some registers with the same text but different tag. These should be excluded from the study","metadata":{}},{"cell_type":"code","source":"inconsistent_texts = (\n    df_train.groupby('text')['target']\n    .nunique()\n    .loc[lambda x: x > 1]  # Find names with more than 1 unique tag\n    .index\n)\n\nprint(f\"There are {len(inconsistent_texts)} texts which are both clasified as disaster and non-disaster\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:16:22.450780Z","iopub.execute_input":"2025-01-28T16:16:22.451123Z","iopub.status.idle":"2025-01-28T16:16:22.469764Z","shell.execute_reply.started":"2025-01-28T16:16:22.451097Z","shell.execute_reply":"2025-01-28T16:16:22.468297Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"such texts must be removed from the original dataframe before continuing the analysis.","metadata":{}},{"cell_type":"code","source":"\n#  Remove rows with inconsistent text classification\ndf_train_cleaned = df_train[~df_train['text'].isin(inconsistent_texts)]\n\nprint(f\"the cleand dataframe has {len(df_train_cleaned)} entries\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T14:56:53.471006Z","iopub.execute_input":"2025-01-28T14:56:53.471349Z","iopub.status.idle":"2025-01-28T14:56:53.481799Z","shell.execute_reply.started":"2025-01-28T14:56:53.471311Z","shell.execute_reply":"2025-01-28T14:56:53.480600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We now see the corresponding distribution of the categories\nplt.figure(figsize=(2, 2))\nsns.histplot(df_train_cleaned, x='target', hue='target', shrink=3, legend=False);\nplt.xticks(rotation=60, ha='right');\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T14:56:53.483268Z","iopub.execute_input":"2025-01-28T14:56:53.483711Z","iopub.status.idle":"2025-01-28T14:56:53.788366Z","shell.execute_reply.started":"2025-01-28T14:56:53.483674Z","shell.execute_reply":"2025-01-28T14:56:53.787330Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The inconsistent tags are a serious problem, but also there might be a problem in case of consistent repeated texts, as they could cause overftting. They will be removed now","metadata":{}},{"cell_type":"code","source":"# Remove duplicates\ndf_train_cleaned = df_train_cleaned.drop_duplicates(subset=['text'])\n\n# Check train data counts. Originally training data has 1490 rows and 50 rows should be dropped. So, I expect 1440 rows left.\nprint(f'Samples in training data after moving duplicates: {df_train_cleaned.shape[0]}')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T14:56:53.789682Z","iopub.execute_input":"2025-01-28T14:56:53.789985Z","iopub.status.idle":"2025-01-28T14:56:53.798410Z","shell.execute_reply.started":"2025-01-28T14:56:53.789960Z","shell.execute_reply":"2025-01-28T14:56:53.797296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We now see the corresponding distribution of the categories\nplt.figure(figsize=(2, 2))\nsns.histplot(df_train_cleaned, x='target', hue='target', shrink=3, legend=False);\nplt.xticks(rotation=60, ha='right');\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T14:56:53.799973Z","iopub.execute_input":"2025-01-28T14:56:53.800292Z","iopub.status.idle":"2025-01-28T14:56:54.070952Z","shell.execute_reply.started":"2025-01-28T14:56:53.800264Z","shell.execute_reply":"2025-01-28T14:56:54.069607Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocess the data","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\nNUM_TRAINING_EXAMPLES = df_train.shape[0]\nTRAIN_SPLIT = 0.8\nVAL_SPLIT = 0.2\nSTEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\nSEED = 412294\nEPOCHS = 2\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2025-01-28T14:56:54.072490Z","iopub.execute_input":"2025-01-28T14:56:54.072836Z","iopub.status.idle":"2025-01-28T14:56:54.078261Z","shell.execute_reply.started":"2025-01-28T14:56:54.072801Z","shell.execute_reply":"2025-01-28T14:56:54.077166Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df_train_cleaned[\"text\"]\ny = df_train_cleaned[\"target\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=SEED)\n\nX_test = df_test[\"text\"]","metadata":{"execution":{"iopub.status.busy":"2025-01-28T14:56:54.080017Z","iopub.execute_input":"2025-01-28T14:56:54.080328Z","iopub.status.idle":"2025-01-28T14:56:54.094601Z","shell.execute_reply.started":"2025-01-28T14:56:54.080301Z","shell.execute_reply":"2025-01-28T14:56:54.093463Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python3 -m pip install wordninja num2words mplcyberpunk spacy==3.3\n\n!python3 -m spacy download en_core_web_lg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:24:10.650750Z","iopub.execute_input":"2025-01-28T16:24:10.651146Z","iopub.status.idle":"2025-01-28T16:28:45.442249Z","shell.execute_reply.started":"2025-01-28T16:24:10.651112Z","shell.execute_reply":"2025-01-28T16:28:45.440735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from spacy import displacy\n\nnlp = spacy.load(\"en_core_web_lg\")\nraw_text=\"\"\"Floods brought on by days of heavy rainfalls have been ravaging countries including Poland, Romania, \n            the Czech Republic and Austria, with thousands of people being evacuated as the death toll rises.\n            Austrian Chancellor Karl Nehammer on Monday said in a post on social media platform X, \n            that was translated by CNBC, that two more people had died following floods in the country. \n            This is in addition to a firefighter who died Sunday.\"\"\"\n\ntext1 = nlp(raw_text)   \ndisplacy.render(text1, style=\"ent\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:28:45.445549Z","iopub.execute_input":"2025-01-28T16:28:45.446077Z","iopub.status.idle":"2025-01-28T16:28:45.516537Z","shell.execute_reply.started":"2025-01-28T16:28:45.446033Z","shell.execute_reply":"2025-01-28T16:28:45.515123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot some most frequent words\ndef frequency_plot(frequency,start,end):\n    frequency = frequency[start:end]\n    plt.figure(figsize=(20,2))\n    plt.bar(frequency.index, frequency)\n    plt.title(f'frequency counts - top {start} to {end} frequent words')\n    plt.xticks(ticks=frequency.index,rotation=90, labels=frequency.index)\n    plt.tick_params(axis='x', which='both', bottom=False, top=False)\n    plt.show()\n\nfrequency = df_train_cleaned['text'].str.split().explode().value_counts()\n\n\nfrequency_plot(frequency,0,40)\nfrequency_plot(frequency, 40,80)\nfrequency_plot(frequency, 80,120)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T14:56:54.096079Z","iopub.execute_input":"2025-01-28T14:56:54.096479Z","iopub.status.idle":"2025-01-28T14:56:55.979638Z","shell.execute_reply.started":"2025-01-28T14:56:54.096443Z","shell.execute_reply":"2025-01-28T14:56:55.978563Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We note first that connecting words (the, a, an, and, to, of,...) are among the most common ones, but they do not add any context to the text. They will be removed. Also, we see that as expected some words start by a \"hashtag\" since it is used for tagging in twitter.\nWords wll then need to be tokenized (for instance using the root). Finally, each tweet will be vectorized.  ","metadata":{}},{"cell_type":"code","source":"print(len(frequency))\nprint(frequency.sort_index()[1000:1010])\nfrequency.sort_index()[21000:21010]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T14:56:55.983026Z","iopub.execute_input":"2025-01-28T14:56:55.983374Z","iopub.status.idle":"2025-01-28T14:56:56.075930Z","shell.execute_reply.started":"2025-01-28T14:56:55.983342Z","shell.execute_reply":"2025-01-28T14:56:56.074833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function for text preprocessing\nimport re\n# nltk\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\ndef text_preprocessing(df):\n    \"\"\"\n    This function does in place replacement of data so it won't return anything\n    \"\"\"\n    # convert to lower cases\n    df['text']=df['text'].str.lower()\n\n    # remove punctuation\n    df['text'] = df['text'].apply(lambda doc: re.sub('\\\\$', ' dollar ',doc)) # eliminates dots ans spaces between numbers\n    df['text'] = df['text'].apply(lambda doc: re.sub('£', ' pound sterling ',doc)) # eliminates dots ans spaces between numbers\n    df['text'] = df['text'].apply(lambda doc: re.sub('€',  ' euro ',doc)) # eliminates dots ans spaces between numbers\n    df['text'] = df['text'].apply(lambda doc: re.sub('%', ' percent ',doc)) # eliminates dots ans spaces between numbers\n\n    df['text'] = df['text'].apply(lambda doc: re.sub(r'(\\d)\\.\\s', r'\\1',doc)) # eliminates dots ans spaces between numbers\n    df['text'] = df['text'].apply(lambda doc: re.sub(r'[^\\w\\s]+', '', doc)) #remove punctuation\n    df['text'] = df['text'].apply(lambda doc: re.sub(\"([0-9])+\", \" a_number \",doc)) # Replaces digits by \"X\"\n    \n\n    \n    # remove stopwords\n    stop_words = nltk.corpus.stopwords.words('english')\n    df['text'] = df['text'].apply(lambda doc: ' '.join([word for word in doc.split() if word not in (stop_words)]))\n\n    \n \n    # remove extra spaces\n    df['text'] = df['text'].apply(lambda doc: re.sub(' +', ' ', doc))\n \n    # stemming\n    porter_stemmer = PorterStemmer()\n    df['text'] = df['text'].apply(lambda doc:  [porter_stemmer.stem(word) for word in word_tokenize(doc) ])\n    df['text'] = df['text'].apply(lambda words: ' '.join(words))\n  \n# take a look at text 0\ntrain_copy = df_train_cleaned.copy()\ntest_copy = df_test.copy()\n\nprint('1st text before preprocessing: \\n',train_copy['text'][0])\ntext_preprocessing(train_copy)\nprint('\\n1st text after preprocessing: \\n',train_copy['text'][0])","metadata":{"execution":{"iopub.status.busy":"2025-01-28T14:56:56.077738Z","iopub.execute_input":"2025-01-28T14:56:56.078210Z","iopub.status.idle":"2025-01-28T14:56:59.857339Z","shell.execute_reply.started":"2025-01-28T14:56:56.078171Z","shell.execute_reply":"2025-01-28T14:56:59.856246Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"frequency = train_copy['text'].str.split().explode().value_counts()\n\nfrequency_plot(frequency,0,40)\nfrequency_plot(frequency, 40,80)\nfrequency_plot(frequency, 80,120)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T14:56:59.858593Z","iopub.execute_input":"2025-01-28T14:56:59.858911Z","iopub.status.idle":"2025-01-28T14:57:01.639206Z","shell.execute_reply.started":"2025-01-28T14:56:59.858884Z","shell.execute_reply":"2025-01-28T14:57:01.638162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.9, ngram_range=(1,2), stop_words=\"english\", sublinear_tf=True)\n\n# DTM: Document Term Matrix\nDTM = tfidf_vectorizer.fit_transform(train_copy.text)\n\nfeature_names = tfidf_vectorizer.get_feature_names_out()\n\nDTM_df = pd.DataFrame(DTM.toarray(), columns=feature_names)\nprint('Document Term Matrix size:', DTM_df.shape)\nDTM_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T14:57:01.640963Z","iopub.execute_input":"2025-01-28T14:57:01.641367Z","iopub.status.idle":"2025-01-28T14:57:02.334378Z","shell.execute_reply.started":"2025-01-28T14:57:01.641329Z","shell.execute_reply":"2025-01-28T14:57:02.333086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import keras.backend as K\n\ndef f1_metric(y_true, y_pred):\n    y_pred = K.round(y_pred)  # Round predictions to 0 or 1\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)  # True positives\n    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)  # False positives\n    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)  # False negatives\n\n    precision = tp / (tp + fp + K.epsilon())  # Add epsilon to avoid division by zero\n    recall = tp / (tp + fn + K.epsilon())\n    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n    return K.mean(f1)  # Average F1 over all batches\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:15:11.531459Z","iopub.execute_input":"2025-01-28T15:15:11.531903Z","iopub.status.idle":"2025-01-28T15:15:11.539624Z","shell.execute_reply.started":"2025-01-28T15:15:11.531869Z","shell.execute_reply":"2025-01-28T15:15:11.538278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import SimpleRNN, Embedding, Dense, LSTM\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LR = 0.001\n# Example dataset\ntexts = train_copy[\"text\"]\nlabels = train_copy[\"target\"]\n\n# Step 1: Preprocessing the data\n# Tokenize the text\ntokenizer = Tokenizer(num_words=1000)  # Limit vocabulary size\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\n\n# Pad the sequences to make them all the same length\nmax_len = 8\nX = pad_sequences(sequences, maxlen=max_len)\n\n\n# Encode labels\nencoder = LabelEncoder()\ny = encoder.fit_transform(labels)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 2: Build the RNN model\nmodel = Sequential([\n    Embedding(input_dim=1000, output_dim=256, input_length=max_len),  # Embedding layer\n    LSTM(16, return_sequences=False, activation='relu'),  # RNN layer with 32 units\n    Dropout(0.2),\n    \n    Dense(16, activation='relu'),  # Output layer for binary classification\n    Dense(32, activation='relu'),  # Output layer for binary classification\n    Dense(1, activation='sigmoid')  # Output layer for binary classification\n])\nmodel.summary()\n\nOPTIMIZER = Adam(learning_rate=LR)\n# Step 3: Compile the model\nmodel.compile(optimizer=OPTIMIZER , loss='binary_crossentropy',metrics=[f1_metric])\n# Step 4: Train the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=4, validation_split=0.2)\n\n# Step 5: Evaluate the model\nloss, f1_val = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {f1_val:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:43:20.467208Z","iopub.execute_input":"2025-01-28T15:43:20.467694Z","iopub.status.idle":"2025-01-28T15:44:53.643726Z","shell.execute_reply.started":"2025-01-28T15:43:20.467659Z","shell.execute_reply":"2025-01-28T15:44:53.642399Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train your own model, fine-tuning BERT","metadata":{}},{"cell_type":"code","source":"def displayConfusionMatrix(y_true, y_pred, dataset):\n    disp = ConfusionMatrixDisplay.from_predictions(\n        y_true,\n        np.argmax(y_pred, axis=1),\n        display_labels=[\"Not Disaster\",\"Disaster\"],\n        cmap=plt.cm.Blues\n    )\n\n    tn, fp, fn, tp = confusion_matrix(y_true, np.argmax(y_pred, axis=1)).ravel()\n    f1_score = tp / (tp+((fn+fp)/2))\n\n    disp.ax_.set_title(\"Confusion Matrix on \" + dataset + \" Dataset -- F1 Score: \" + str(f1_score.round(2)))\n","metadata":{"execution":{"iopub.status.busy":"2025-01-28T15:31:04.787624Z","iopub.execute_input":"2025-01-28T15:31:04.787992Z","iopub.status.idle":"2025-01-28T15:31:04.794444Z","shell.execute_reply.started":"2025-01-28T15:31:04.787962Z","shell.execute_reply":"2025-01-28T15:31:04.793297Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_train = model.predict(X_train)\n\ndisplayConfusionMatrix(y_train, y_pred_train, \"Training\")","metadata":{"execution":{"iopub.status.busy":"2025-01-28T15:31:07.718946Z","iopub.execute_input":"2025-01-28T15:31:07.719341Z","iopub.status.idle":"2025-01-28T15:31:08.716663Z","shell.execute_reply.started":"2025-01-28T15:31:07.719307Z","shell.execute_reply":"2025-01-28T15:31:08.715537Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_val = classifier.predict(X_val)\n\ndisplayConfusionMatrix(y_val, y_pred_val, \"Validation\")","metadata":{"execution":{"iopub.status.busy":"2025-01-28T14:57:53.690374Z","iopub.execute_input":"2025-01-28T14:57:53.690839Z","iopub.status.idle":"2025-01-28T14:57:53.759732Z","shell.execute_reply.started":"2025-01-28T14:57:53.690784Z","shell.execute_reply":"2025-01-28T14:57:53.758325Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate the submission file \n\nFor each tweets in the test set, we predict if the given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\nThe `submission.csv` file uses the following format:\n`id,target`","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2025-01-28T14:57:53.760834Z","iopub.status.idle":"2025-01-28T14:57:53.761247Z","shell.execute_reply.started":"2025-01-28T14:57:53.761063Z","shell.execute_reply":"2025-01-28T14:57:53.761083Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission[\"target\"] = np.argmax(classifier.predict(X_test), axis=1)","metadata":{"execution":{"iopub.status.busy":"2025-01-28T14:57:53.762524Z","iopub.status.idle":"2025-01-28T14:57:53.762924Z","shell.execute_reply.started":"2025-01-28T14:57:53.762750Z","shell.execute_reply":"2025-01-28T14:57:53.762770Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission.describe()","metadata":{"execution":{"iopub.status.busy":"2025-01-28T14:57:53.764166Z","iopub.status.idle":"2025-01-28T14:57:53.764682Z","shell.execute_reply.started":"2025-01-28T14:57:53.764433Z","shell.execute_reply":"2025-01-28T14:57:53.764458Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2025-01-28T14:57:53.766290Z","iopub.status.idle":"2025-01-28T14:57:53.766823Z","shell.execute_reply.started":"2025-01-28T14:57:53.766572Z","shell.execute_reply":"2025-01-28T14:57:53.766595Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"https://www.kaggle.com/code/aletbm/nlp-with-disaster-tweets","metadata":{}}]}